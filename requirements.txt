streamlit>=1.24.0
llama-cpp-python>=0.2.11
langchain>=0.1.0
pydantic>=2.0.0
requests>=2.31.0
# 
# import torch
# if torch.backends.mps.is_available():
#     mps_device = torch.device("mps")
#     x = torch.ones(1, device=mps_device)
#     print (x)
# else:
#     print ("MPS device not found.")
# The output should show:
# 
# tensor([1.], device='mps:0')
# 

streamlit
langchain
langchain-community
# langchain-ollama # Commented out as we are switching to Hugging Face
# ollama # Commented out as we are switching to Hugging Face
pypdf2
pandas
transformers
# torch # PyTorch is a dependency for Transformers - specific installation instructions above
# Ensure you install PyTorch, torchvision, and torchaudio according to your environment (pip/conda) and needs (stable/nightly)
# The line below is a general placeholder if you let pip manage it, but for MPS, specific index URLs might be better.
torch >= 1.12 # Minimum version that generally supports MPS, but latest stable or nightly is better.
torchvision
torchaudio
accelerate # Often needed for optimal performance with Transformers
# bitsandbytes # Required for 4-bit/8-bit quantization on compatible hardware (mainly CUDA)
modelscope # For model management and deployment

# Hugging Face Model Download (GGUF format)
# https://huggingface.co/Qwen/Qwen3-8B-GGUF/resolve/main/Qwen3-8B-Q4_K_M.gguf?download=true
#